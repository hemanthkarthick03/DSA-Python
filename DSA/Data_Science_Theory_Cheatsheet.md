# Data Science Theory Cheatsheet ğŸ“Š

## Statistics Fundamentals ğŸ“ˆ

### Descriptive Statistics ğŸ“‹
- **Mean (Î¼)** ğŸ¯: Average value of dataset
- **Median** ğŸ : Middle value when data is sorted
- **Mode** ğŸ”„: Most frequently occurring value
- **Standard Deviation (Ïƒ)** ğŸ“: Measure of data spread
- **Variance (ÏƒÂ²)** ğŸ“: Square of standard deviation
- **Skewness** âš–ï¸: Measure of asymmetry in distribution
- **Kurtosis** ğŸ”ï¸: Measure of tail heaviness in distribution

### Probability Distributions ğŸ²
- **Normal Distribution** ğŸ””: Bell-shaped, symmetric (68-95-99.7 rule)
- **Binomial Distribution** ğŸª™: Success/failure trials (coin flips)
- **Poisson Distribution** â°: Rate of events over time/space
- **Exponential Distribution** âš¡: Time between events
- **Uniform Distribution** ğŸ“: All outcomes equally likely
- **Chi-Square Distribution** âœ–ï¸: Sum of squared normal variables

### Central Limit Theorem ğŸ¯
- **Key Point**: Sample means approach normal distribution as n increases
- **Magic Number**: n â‰¥ 30 for CLT to apply
- **Formula**: Ïƒ_xÌ„ = Ïƒ/âˆšn (standard error)

## Hypothesis Testing ğŸ§ª

### Test Types ğŸ”¬
- **One-sample t-test** 1ï¸âƒ£: Compare sample mean to population mean
- **Two-sample t-test** 2ï¸âƒ£: Compare two group means
- **Paired t-test** ğŸ‘¥: Compare before/after measurements
- **Chi-square test** âœ–ï¸: Test independence of categorical variables
- **ANOVA** ğŸ­: Compare multiple group means
- **Mann-Whitney U** ğŸš«ğŸ“Š: Non-parametric alternative to t-test

### Key Concepts ğŸ”‘
- **Null Hypothesis (Hâ‚€)** âŒ: No effect/difference exists
- **Alternative Hypothesis (Hâ‚)** âœ…: Effect/difference exists
- **p-value** ğŸ“Š: Probability of observing data if Hâ‚€ is true
- **Alpha (Î±)** ğŸ¯: Significance level (usually 0.05)
- **Type I Error** ğŸš¨: False positive (reject true Hâ‚€)
- **Type II Error** ğŸ˜´: False negative (fail to reject false Hâ‚€)
- **Power** ğŸ’ª: Probability of correctly rejecting false Hâ‚€

## Machine Learning Fundamentals ğŸ¤–

### Learning Types ğŸ“š
- **Supervised Learning** ğŸ‘¨â€ğŸ«: Learn from labeled data
  - Classification ğŸ·ï¸: Predict categories
  - Regression ğŸ“ˆ: Predict continuous values
- **Unsupervised Learning** ğŸ”: Find patterns in unlabeled data
  - Clustering ğŸ¯: Group similar data points
  - Dimensionality Reduction ğŸ“‰: Reduce feature space
- **Reinforcement Learning** ğŸ®: Learn through rewards/penalties

### Bias-Variance Tradeoff âš–ï¸
- **Bias** ğŸ¯: Error from oversimplifying model
- **Variance** ğŸ“Š: Error from sensitivity to training data
- **Underfitting** ğŸ“‰: High bias, low variance
- **Overfitting** ğŸ“ˆ: Low bias, high variance
- **Sweet Spot** ğŸ¯: Balance both for optimal performance

### Cross-Validation ğŸ”„
- **k-Fold CV** ğŸ“: Split data into k parts, train on k-1, test on 1
- **Leave-One-Out** 1ï¸âƒ£: Special case where k = n
- **Stratified CV** ğŸ“Š: Maintain class proportions in splits
- **Time Series CV** â°: Respect temporal order in splits

## Classification Algorithms ğŸ·ï¸

### Linear Models ğŸ“
- **Logistic Regression** ğŸ“ˆ: Uses sigmoid function for probability
- **Linear Discriminant Analysis** ğŸ“: Finds linear decision boundary
- **Support Vector Machine** âš”ï¸: Finds optimal separating hyperplane

### Tree-Based Models ğŸŒ³
- **Decision Tree** ğŸŒ²: Recursive binary splits
- **Random Forest** ğŸŒ²ğŸŒ²ğŸŒ²: Ensemble of decision trees
- **Gradient Boosting** ğŸš€: Sequential weak learners
- **XGBoost** âš¡: Optimized gradient boosting

### Distance-Based Models ğŸ“
- **k-Nearest Neighbors** ğŸ‘¥: Classify based on k closest points
- **Naive Bayes** ğŸ²: Assumes feature independence

### Neural Networks ğŸ§ 
- **Perceptron** âš¡: Single layer linear classifier
- **Multi-layer Perceptron** ğŸ§ : Multiple hidden layers
- **Deep Learning** ğŸ—ï¸: Many hidden layers

## Regression Algorithms ğŸ“ˆ

### Linear Regression Family ğŸ“
- **Simple Linear** â¡ï¸: y = mx + b
- **Multiple Linear** â¡ï¸â¡ï¸: y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ...
- **Polynomial** ğŸŒŠ: Non-linear relationships
- **Ridge Regression** ğŸ”ï¸: L2 regularization (Î»Î£Î²Â²)
- **Lasso Regression** ğŸ¯: L1 regularization (Î»Î£|Î²|)
- **Elastic Net** ğŸ•¸ï¸: Combines L1 + L2 regularization

### Non-Linear Models ğŸŒŠ
- **Decision Tree Regression** ğŸŒ³: Recursive splits for continuous target
- **Random Forest Regression** ğŸŒ²ğŸŒ²ğŸŒ²: Ensemble averaging
- **Support Vector Regression** âš”ï¸: Uses kernel trick for non-linearity

## Clustering Algorithms ğŸ¯

### Partitioning Methods ğŸ“Š
- **k-Means** ğŸ¯: Minimize within-cluster sum of squares
- **k-Medoids** ğŸ : Use actual data points as centers
- **Fuzzy c-Means** ğŸŒ«ï¸: Soft clustering with membership degrees

### Hierarchical Methods ğŸŒ³
- **Agglomerative** â¬†ï¸: Bottom-up clustering
- **Divisive** â¬‡ï¸: Top-down clustering
- **Dendrogram** ğŸŒ³: Tree visualization of clusters

### Density-Based Methods ğŸŒŠ
- **DBSCAN** ğŸ”: Density-based spatial clustering
- **OPTICS** ğŸ‘ï¸: Ordering points to identify clustering structure

## Dimensionality Reduction ğŸ“‰

### Linear Methods ğŸ“
- **Principal Component Analysis (PCA)** ğŸ¯: Maximize variance
- **Linear Discriminant Analysis (LDA)** ğŸ“: Maximize class separation
- **Independent Component Analysis (ICA)** ğŸ”„: Find independent sources

### Non-Linear Methods ğŸŒŠ
- **t-SNE** ğŸ—ºï¸: Preserve local neighborhood structure
- **UMAP** ğŸš€: Uniform manifold approximation
- **Kernel PCA** ğŸ”„: PCA in higher dimensional space

## Model Evaluation Metrics ğŸ“Š

### Classification Metrics ğŸ·ï¸
- **Accuracy** ğŸ¯: (TP + TN) / Total
- **Precision** ğŸ”: TP / (TP + FP) - "How many selected are relevant?"
- **Recall (Sensitivity)** ğŸ“¡: TP / (TP + FN) - "How many relevant are selected?"
- **Specificity** ğŸ›¡ï¸: TN / (TN + FP) - "How many irrelevant are rejected?"
- **F1-Score** âš–ï¸: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
- **AUC-ROC** ğŸ“ˆ: Area under ROC curve
- **Confusion Matrix** ğŸ¤”: 2Ã—2 table of predictions vs actual

### Regression Metrics ğŸ“ˆ
- **Mean Absolute Error (MAE)** ğŸ“: Average absolute differences
- **Mean Squared Error (MSE)** ğŸ“: Average squared differences
- **Root Mean Squared Error (RMSE)** âˆšğŸ“: Square root of MSE
- **R-squared (RÂ²)** ğŸ“Š: Proportion of variance explained
- **Adjusted RÂ²** ğŸ”§: RÂ² adjusted for number of features

## Feature Engineering ğŸ”§

### Feature Selection ğŸ¯
- **Filter Methods** ğŸ”: Statistical tests (correlation, chi-square)
- **Wrapper Methods** ğŸ“¦: Use ML algorithm (forward/backward selection)
- **Embedded Methods** ğŸ—ï¸: Built into algorithm (Lasso, tree importance)

### Feature Transformation ğŸ”„
- **Scaling** ğŸ“: StandardScaler, MinMaxScaler, RobustScaler
- **Encoding** ğŸ·ï¸: One-hot, Label, Target encoding
- **Binning** ğŸ“Š: Convert continuous to categorical
- **Log Transform** ğŸ“ˆ: Handle skewed distributions
- **Polynomial Features** ğŸŒŠ: Create interaction terms

## Data Preprocessing ğŸ§¹

### Missing Data Handling ğŸ•³ï¸
- **Deletion** âŒ: Remove rows/columns with missing values
- **Mean/Median Imputation** ğŸ“Š: Fill with central tendency
- **Mode Imputation** ğŸ”„: Fill with most frequent value
- **Forward/Backward Fill** â¡ï¸â¬…ï¸: Use previous/next value
- **KNN Imputation** ğŸ‘¥: Use similar observations
- **Multiple Imputation** ğŸ”„ğŸ”„: Create multiple complete datasets

### Outlier Detection ğŸš¨
- **Z-Score Method** ğŸ“Š: |z| > 3 (assuming normal distribution)
- **IQR Method** ğŸ“¦: Outside Q1 - 1.5Ã—IQR or Q3 + 1.5Ã—IQR
- **Isolation Forest** ğŸŒ³: Anomaly detection algorithm
- **Local Outlier Factor** ğŸ¯: Density-based outlier detection

## Time Series Analysis â°

### Components ğŸ§©
- **Trend** ğŸ“ˆ: Long-term direction
- **Seasonality** ğŸ”„: Regular patterns (daily, weekly, yearly)
- **Cyclical** ğŸŒŠ: Irregular long-term patterns
- **Noise** ğŸ“»: Random variation

### Stationarity ğŸ“Š
- **Stationary Series** âš–ï¸: Constant mean, variance, covariance
- **Augmented Dickey-Fuller Test** ğŸ§ª: Test for stationarity
- **Differencing** â–: Remove trend to achieve stationarity

### Forecasting Models ğŸ”®
- **ARIMA** ğŸ“ˆ: AutoRegressive Integrated Moving Average
- **SARIMA** ğŸ”„: Seasonal ARIMA
- **Exponential Smoothing** ğŸ“‰: Weighted averages
- **Prophet** ğŸ”®: Facebook's forecasting tool

## Deep Learning Basics ğŸ§ 

### Neural Network Components âš¡
- **Neuron** ğŸ”µ: Basic processing unit
- **Weights** âš–ï¸: Connection strengths
- **Bias** ğŸ¯: Threshold adjustment
- **Activation Functions** ğŸ”¥: ReLU, Sigmoid, Tanh
- **Loss Function** ğŸ“‰: Measure prediction error
- **Optimizer** ğŸš€: Update weights (SGD, Adam)

### Network Types ğŸ—ï¸
- **Feedforward** â¡ï¸: Information flows forward only
- **Convolutional (CNN)** ğŸ–¼ï¸: For image processing
- **Recurrent (RNN)** ğŸ”„: For sequential data
- **Long Short-Term Memory (LSTM)** ğŸ§ : Advanced RNN
- **Transformer** ğŸ”„: Attention-based architecture

## Ensemble Methods ğŸ­

### Bagging ğŸ‘œ
- **Bootstrap Aggregating** ğŸ’: Train on bootstrap samples
- **Random Forest** ğŸŒ²ğŸŒ²ğŸŒ²: Bagging + random feature selection
- **Extra Trees** ğŸŒ³: Extremely randomized trees

### Boosting ğŸš€
- **AdaBoost** ğŸ“ˆ: Adaptive boosting
- **Gradient Boosting** ğŸ“Š: Sequential error correction
- **XGBoost** âš¡: Extreme gradient boosting
- **LightGBM** ğŸ’¡: Light gradient boosting

### Stacking ğŸ“š
- **Meta-learner** ğŸ“: Learn from base model predictions
- **Blending** ğŸŒ€: Simple averaging of predictions

## Model Selection & Tuning ğŸ›ï¸

### Hyperparameter Tuning ğŸ”§
- **Grid Search** ğŸ”: Exhaustive search over parameter grid
- **Random Search** ğŸ²: Random sampling of parameters
- **Bayesian Optimization** ğŸ§ : Smart parameter search
- **Genetic Algorithm** ğŸ§¬: Evolution-based optimization

### Model Selection Criteria ğŸ“Š
- **AIC** ğŸ“ˆ: Akaike Information Criterion
- **BIC** ğŸ“Š: Bayesian Information Criterion
- **Cross-Validation Score** ğŸ”„: Average performance across folds
- **Learning Curves** ğŸ“ˆ: Plot training vs validation error

## Statistical Concepts ğŸ“Š

### Correlation vs Causation âš–ï¸
- **Correlation** ğŸ”—: Variables move together
- **Causation** â¡ï¸: One variable causes another
- **Confounding Variables** ğŸŒªï¸: Hidden factors affecting both
- **Simpson's Paradox** ğŸ”„: Trend reverses when data is grouped

### Sampling ğŸ¯
- **Simple Random** ğŸ²: Each item has equal probability
- **Stratified** ğŸ“Š: Maintain population proportions
- **Systematic** ğŸ“: Every kth item
- **Cluster** ğŸ¯: Sample entire groups
- **Convenience** ğŸ¤·: Easy to collect samples

### Experimental Design ğŸ§ª
- **Control Group** ğŸ›¡ï¸: No treatment applied
- **Treatment Group** ğŸ’Š: Receives intervention
- **Randomization** ğŸ²: Random assignment to groups
- **Blinding** ğŸ‘ï¸: Participants don't know group assignment
- **Double-Blind** ğŸ‘ï¸ğŸ‘ï¸: Neither participants nor researchers know

## Business Intelligence ğŸ’¼

### KPIs & Metrics ğŸ“Š
- **Conversion Rate** ğŸ’°: Visitors who take desired action
- **Customer Lifetime Value (CLV)** ğŸ’: Total value from customer
- **Churn Rate** ğŸ“‰: Rate of customer attrition
- **A/B Testing** ğŸ§ª: Compare two versions
- **Cohort Analysis** ğŸ‘¥: Track user groups over time

### Data Quality ğŸ†
- **Completeness** âœ…: No missing values
- **Accuracy** ğŸ¯: Values are correct
- **Consistency** ğŸ”„: Same format across dataset
- **Timeliness** â°: Data is up-to-date
- **Validity** âœ”ï¸: Data follows business rules

## Ethics & Bias ğŸ¤

### Types of Bias ğŸš¨
- **Selection Bias** ğŸ¯: Non-representative sample
- **Confirmation Bias** âœ…: Seeking confirming evidence
- **Survivorship Bias** ğŸ†: Only considering successful cases
- **Algorithmic Bias** ğŸ¤–: Unfair ML model decisions
- **Sampling Bias** ğŸ“Š: Systematic error in sample selection

### Fairness Metrics âš–ï¸
- **Demographic Parity** ğŸ‘¥: Equal positive rates across groups
- **Equalized Odds** âš–ï¸: Equal TPR and FPR across groups
- **Individual Fairness** ğŸ‘¤: Similar individuals get similar outcomes

## Quick Reference Formulas ğŸ“

### Statistics ğŸ“Š
- **Standard Error**: SE = Ïƒ/âˆšn
- **Confidence Interval**: xÌ„ Â± z(Î±/2) Ã— SE
- **Cohen's d**: d = (Î¼â‚ - Î¼â‚‚)/Ïƒ_pooled
- **Correlation**: r = Î£(x-xÌ„)(y-È³)/âˆš[Î£(x-xÌ„)Â²Î£(y-È³)Â²]

### Machine Learning ğŸ¤–
- **Precision**: TP/(TP+FP)
- **Recall**: TP/(TP+FN)
- **F1-Score**: 2Ã—(PrecisionÃ—Recall)/(Precision+Recall)
- **Accuracy**: (TP+TN)/(TP+TN+FP+FN)

### Information Theory ğŸ“¡
- **Entropy**: H(X) = -Î£p(x)logâ‚‚p(x)
- **Information Gain**: IG = H(parent) - Î£(|child|/|parent|)Ã—H(child)
- **Gini Impurity**: 1 - Î£p(x)Â²

## Memory Tricks ğŸ§ 

### Statistical Tests ğŸ§ª
- **t-test** â˜•: "Tea for Two" (compare means)
- **Chi-square** âœ–ï¸: "Chi-squared for Categories"
- **ANOVA** ğŸ­: "Analysis of Variance for Multiple groups"
- **F-test** ğŸ: "F for comparing Variances"

### ML Algorithm Selection ğŸ¯
- **Linear** ğŸ“: Simple, interpretable, fast
- **Tree** ğŸŒ³: Non-linear, interpretable, handles mixed data
- **SVM** âš”ï¸: High-dimensional, kernel trick
- **Neural** ğŸ§ : Complex patterns, needs lots of data
- **Ensemble** ğŸ­: Better performance, less interpretable

### Remember the 3 V's of Big Data ğŸ“Š
- **Volume** ğŸ“ˆ: Amount of data
- **Velocity** âš¡: Speed of data
- **Variety** ğŸŒˆ: Types of data

This cheatsheet provides quick reference points for mastering data science theory! ğŸš€